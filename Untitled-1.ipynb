{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#My first notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_marvel_movies():\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_Marvel_Cinematic_Universe_films\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    tables = soup.find_all('table', class_='wikitable')\n",
    "    \n",
    "    all_data = []\n",
    "    for i, table in enumerate(tables[:7]):  # Phases 1-6 tables\n",
    "        headers = [header.text.strip() for header in table.find_all('th')]\n",
    "        rows = table.find_all('tr')\n",
    "        data = []\n",
    "        columns = ['film', 'release_date', 'director', 'writer', 'producer', \"status\"]\n",
    "        \n",
    "        for row in rows:\n",
    "            cols = row.find_all(['td', 'th'])\n",
    "            cols = [ele.text.strip() for ele in cols]\n",
    "            if len(cols) < len(columns):\n",
    "                cols.extend([None] * (len(columns) - len(cols)))\n",
    "            elif len(cols) > len(columns):\n",
    "                cols = cols[:len(columns)]\n",
    "            data.append(cols)\n",
    "        \n",
    "        df = pd.DataFrame(data[1:], columns=columns)  \n",
    "        df['phase'] = f\"Phase {i+1}\"\n",
    "        all_data.append(df)\n",
    "    \n",
    "    return pd.concat(all_data, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Clean Movie Data\n",
    "def clean_movie_data(movies_df):\n",
    "    movies_df['producer'] = movies_df['producer'].fillna(method='ffill')\n",
    "    movies_df['status'] = movies_df['status'].fillna(method='ffill')\n",
    "    \n",
    "    def remove_references(text):\n",
    "        return re.sub(r'\\s*\\[\\s*\\d+\\s*\\]', '', text)\n",
    "    \n",
    "    movies_df_cleaned = movies_df.applymap(lambda cell: remove_references(cell) if isinstance(cell, str) else cell)\n",
    "    \n",
    "    movies_df_cleaned['release_date'] = pd.to_datetime(\n",
    "        movies_df_cleaned['release_date'].str.extract(r'\\((.*?)\\)')[0], errors='coerce'\n",
    "    )\n",
    "    movies_df_cleaned['release_date'] = movies_df_cleaned['release_date'].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    return movies_df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3 : Scrape Character Data\n",
    "def fetch_omdb_data(film_name):\n",
    "    url = f'http://www.omdbapi.com/?t={film_name}&apikey=982d07c3'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {\"Title\": film_name, \"Error\": \"Data not found\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Scrape Characters Data\n",
    "def scrape_characters_data():\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_Marvel_Cinematic_Universe_films\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = None\n",
    "    \n",
    "    for tbl in soup.find_all('table', class_='wikitable'):\n",
    "        caption = tbl.find('caption')\n",
    "        if caption and \"Recurring cast and characters of Marvel Cinematic Universe films\" in caption.get_text():\n",
    "            table = tbl\n",
    "            break\n",
    "\n",
    "    if table:\n",
    "        headers = [th.get_text(strip=True) for th in table.find_all('tr')[0].find_all('th')]\n",
    "        rows = []\n",
    "        \n",
    "        for tr in table.find_all('tr')[1:]:\n",
    "            row = []\n",
    "            for td in tr.find_all(['th', 'td']):\n",
    "                colspan = td.get('colspan')\n",
    "                if colspan:\n",
    "                    colspan = int(colspan)\n",
    "                    row.extend([td.get_text(separator=\" \", strip=True)] * colspan)\n",
    "                else:\n",
    "                    row.append(td.get_text(separator=\" \", strip=True))\n",
    "\n",
    "            while len(row) < len(headers):\n",
    "                row.append(None)\n",
    "\n",
    "            if len(row) > len(headers):\n",
    "                row = row[:len(headers)]  # Truncate to match header length\n",
    "            \n",
    "            rows.append(row)\n",
    "\n",
    "        return pd.DataFrame(rows, columns=headers)\n",
    "    else:\n",
    "        print(\"Table with specified caption not found.\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Fetch Movie Data from OMDB API\n",
    "\n",
    "def fetch_omdb_data(film_name):\n",
    "    url = f'http://www.omdbapi.com/?t={film_name}&apikey=982d07c3'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return {\"Title\": film_name, \"Error\": \"Data not found\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Upload DataFrames to S3 with specific folder paths\n",
    "def upload_to_s3(df, file_name):\n",
    "    s3 = boto3.client('s3')\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "    s3.put_object(Bucket='marvelmovie-bucket', Key=f\"{file_name}\", Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda handler function to call each step\n",
    "def lambda_handler(event, context):\n",
    "    # Step 1: Scrape movie titles from Wikipedia\n",
    "    movies_df = scrape_marvel_movies()\n",
    "    \n",
    "    # Step 2: Clean the scraped movie data\n",
    "    movies_df_cleaned = clean_movie_data(movies_df)\n",
    "    \n",
    "    # Step 3: Fetch additional movie data from OMDB API\n",
    "    omdb_results = []\n",
    "    for film in movies_df_cleaned['film']:\n",
    "        omdb_data = fetch_omdb_data(film)\n",
    "        omdb_results.append(omdb_data)\n",
    "    \n",
    "    # Convert OMDB results to DataFrame\n",
    "    omdb_df = pd.DataFrame(omdb_results)\n",
    "\n",
    "    # Step 4: Scrape characters data\n",
    "    characters_df = scrape_characters_data()\n",
    "\n",
    "    # Upload DataFrames to S3 with specified folder paths\n",
    "    upload_to_s3(movies_df_cleaned, 'movies.csv')\n",
    "    upload_to_s3(omdb_df, 'omdb.csv')\n",
    "    upload_to_s3(characters_df, 'characters.csv')\n",
    "    \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': 'Data uploaded successfully!'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
